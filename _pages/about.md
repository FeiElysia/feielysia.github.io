---
permalink: /
title: "Junjie Fei's Homepage"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am currently a PhD student at the King Abdullah University of Science and Technology (KAUST), under the supervision of [Prof. Mohamed Elhoseiny](https://www.mohamed-elhoseiny.com/). Before that, I obtained my BS and MS degrees from Chongqing University and Xiamen University, respectively. I also gained valuable research experience as a visiting student / research assistant at SUSTech VIP Lab and [KAUST Vision CAIR](https://cemse.kaust.edu.sa/vision-cair). Please refer to my [CV](https://feielysia.github.io/images/CV.pdf) for more details.

My recent research interests are focused on vision-language multimodal learning. Feel free to drop me an email at [junjiefei@outlook.com](mailto:junjiefei@outlook.com) / [junjie.fei@kaust.edu.sa](mailto:junjie.fei@kaust.edu.sa) if you are interested in collaborating.

News
======
* [2025/06] 2 papers have been accepted by ICCV 2025!
* [2025/02] 1 paper has been accepted by CVPR 2025!
* [2024/08] Join KAUST as a PhD student!
* [2023/07] 1 paper has been accepted by ICCV 2023!
* [2023/04] Project [*Caption Anything*](https://github.com/ttengwang/Caption-Anything) is publicly released!

Research
======
(* equal contribution)

<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto"><tbody>

  <tr onmouseout="iadsurvey_stop()" onmouseover="iadsurvey_start()">
    <td style="padding:20px;width:25%;vertical-align:middle;border:0px;border-spacing:0px;border-collapse:separate">
        <div class="one">
          <img src='images/Kestrel.jpg' width="400">
        </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle;border:0px;border-spacing:0px;border-collapse:separate">
        <a href="https://arxiv.org/abs/2405.18937">
          <papertitle>Kestrel: Point Grounding Multimodal LLM for Part-Aware 3D Vision-Language Understanding</papertitle>
        </a>
        <br>
        <strong>Junjie Fei*</strong>, Mahmoud Ahmed*, Jian Ding, Eslam Mohamed Bakr, Mohamed Elhoseiny
        <br>
        <em>ICCV</em>, 2025
        <br>
        <a href="https://feielysia.github.io/Kestrel.github.io/">project</a>
        /
        <a href="https://arxiv.org/pdf/2405.18937">paper</a>
        <p></p>
        <p>
        Kestrel is a part-aware point grounding 3D MLLM, capable of comprehending and generating language and locating the position of the object and its materials at the part level.
        </p>
    </td>
  </tr>

  <!-- <tr onmouseout="iadsurvey_stop()" onmouseover="iadsurvey_start()">
    <td style="padding:20px;width:25%;vertical-align:middle;border:0px;border-spacing:0px;border-collapse:separate">
        <div class="one">
          <img src='images/WikiAutoGen.png' width="400">
        </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle;border:0px;border-spacing:0px;border-collapse:separate">
        <a href="https://arxiv.org/abs/2503.19065">
          <papertitle>Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents</papertitle>
        </a>
        <br>
        Zhongyu Yang*, Jun Chen*, Dannong Xu, <strong>Junjie Fei</strong>, Xiaoqian Shen, Liangbing Zhao, Chun-Mei Feng, Mohamed Elhoseiny
        <br>
        <em>ICCV</em>, 2025
        <br>
        <a href="https://wikiautogen.github.io/">project</a>
        /
        <a href="https://github.com/01yzzyu/wikiautogen">code</a>
        /
        <a href="https://arxiv.org/pdf/2503.19065">paper</a>
        <p></p>
        <p>
        WikiAutoGen is a novel system for automated multimodal Wikipedia-style article generation, retrieving and integrating relevant images alongside text to enhance both the depth and visual appeal of the generated content.
    </td>
  </tr>

  <tr onmouseout="iadsurvey_stop()" onmouseover="iadsurvey_start()">
    <td style="padding:20px;width:25%;vertical-align:middle;border:0px;border-spacing:0px;border-collapse:separate">
        <div class="one">
          <img src='images/Dochaystack.png' width="400">
        </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle;border:0px;border-spacing:0px;border-collapse:separate">
        <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Document_Haystacks__Vision-Language_Reasoning_Over_Piles_of_1000_Documents_CVPR_2025_paper.html">
          <papertitle>Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents</papertitle>
        </a>
        <br>
        Jun Chen*, Dannong Xu*, <strong>Junjie Fei*</strong>, Chun-Mei Feng, Mohamed Elhoseiny
        <br>
        <em>CVPR</em>, 2025
        <br>
        <a href="https://github.com/Vision-CAIR/dochaystacks">code</a>
        /
        <a href="https://arxiv.org/pdf/2411.16740">paper</a>
        /
        <a href="https://huggingface.co/datasets/DanielXu0208/Document_Haystacks">benchmark</a>
        <p></p>
        <p>
        The Document Haystack Benchmarks aim to evaluate the performance of VLMs on large-scale visual document retrieval and understanding.
        </p>
    </td>
  </tr> -->

  <tr onmouseout="iadsurvey_stop()" onmouseover="iadsurvey_start()">
    <td style="padding:20px;width:25%;vertical-align:middle;border:0px;border-spacing:0px;border-collapse:separate">
        <div class="one">
          <img src='images/ViECap.jpg' width="400">
        </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle;border:0px;border-spacing:0px;border-collapse:separate">
        <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fei_Transferable_Decoding_with_Visual_Entities_for_Zero-Shot_Image_Captioning_ICCV_2023_paper.html">
          <papertitle>Transferable Decoding with Visual Entities for Zero‑Shot Image Captioning</papertitle>
        </a>
        <br>
        <strong>Junjie Fei*</strong>, Teng Wang*, Jinrui Zhang, Zhenyu He, Chengjie Wang, Feng Zheng
        <br>
        <em>ICCV</em>, 2023
        <br>
        <a href="https://github.com/FeiElysia/ViECap">code</a>
        /
        <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fei_Transferable_Decoding_with_Visual_Entities_for_Zero-Shot_Image_Captioning_ICCV_2023_paper.pdf">paper</a>
        <p></p>
        <p>
        Improving the transferability of zero-shot captioning for out-of-domain images by addressing the modality bias and object hallucination that arise when adapting pre-trained vision-language models and large language models.
        </p>
    </td>
  </tr>

  <tr onmouseout="iadsurvey_stop()" onmouseover="iadsurvey_start()">
    <td style="padding:20px;width:25%;vertical-align:middle;border:0px;border-spacing:0px;border-collapse:separate">
        <div class="one">
          <img src='images/CAT.jpg' width="400">
        </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle;border:0px;border-spacing:0px;border-collapse:separate">
        <a href="https://arxiv.org/abs/2305.02677">
          <papertitle>Caption Anything: Interactive Image Description with Diverse Multimodal Controls</papertitle>
        </a>
        <br>
        Teng Wang*, Jinrui Zhang*, <strong>Junjie Fei*</strong>, Hao Zheng, Yunlong Tang, Zhe Li, Mingqi Gao, Shanshan Zhao
        <br>
        <em>arXiv</em>, 2023
        <br>
        <a href="https://github.com/ttengwang/Caption-Anything">code</a>
        /
        <a href="https://arxiv.org/pdf/2305.02677.pdf">paper</a>
        /
        <a href="https://huggingface.co/spaces/TencentARC/Caption-Anything">demo</a>
        <p></p>
        <p>
        Caption Anything is an interactive image‑to‑text generative tool that can generate diverse descriptions for any user-specified object within an image, providing a variety of language styles and visual controls to cater to diverse user preferences.
        </p>
    </td>
  </tr>

  <!-- <tr onmouseout="iadsurvey_stop()" onmouseover="iadsurvey_start()">
    <td style="padding:20px;width:25%;vertical-align:middle;border:0px;border-spacing:0px;border-collapse:separate">
        <div class="one">
          <img src='images/U_Net.jpg' width="400">
        </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle;border:0px;border-spacing:0px;border-collapse:separate">
        <a href="https://ieeexplore.ieee.org/document/9896914">
          <papertitle>Hybrid Microwave Imaging of 3-D Objects Using LSM and BIM Aided by a CNN U-Net</papertitle>
        </a>
        <br>
        Feng Han, Miao Zhong, <strong>Junjie Fei</strong>
        <br>
        <em>IEEE Transactions on Geoscience and Remote Sensing</em> (2 Year IF: 8.125, ranking: 42/708)
        <br>
        <a href="https://ieeexplore.ieee.org/document/9896914">paper</a>
        <p></p>
        <p>
        An efficient and accurate 3-D quantitative hybrid microwave imaging method, which incorporates 3D U-Net to further refine the reconstructed object.
        </p>
    </td>
  </tr>

  <tr onmouseout="iadsurvey_stop()" onmouseover="iadsurvey_start()">
    <td style="padding:20px;width:25%;vertical-align:middle;border:0px;border-spacing:0px;border-collapse:separate">
        <div class="one">
          <img src='images/ResU_Net.jpg' width="400">
        </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle;border:0px;border-spacing:0px;border-collapse:separate">
        <a href="https://ieeexplore.ieee.org/document/9670652">
          <papertitle>Fast 3-D Electromagnetic Full-Wave Inversion of Dielectric Anisotropic Objects Based on ResU-Net Enhanced by Variational Born Iterative Method</papertitle>
        </a>
        <br>
        <strong>Junjie Fei</strong>, Yanjin Chen, Miao Zhong, Feng Han
        <br>
        <em>IEEE Transactions on Antennas and Propagation</em> (2 Year IF: 4.824, ranking: 71/708)
        <br>
        <a href="https://ieeexplore.ieee.org/document/9670652">paper</a>
        <p></p>
        <p>
        ResU-Net is proposed to directly reconstruct 3-D anisotropic objects from the received electromagnetic field data.
        </p>
    </td>
  </tr> -->

</tbody></table>

Academic Services
======
Conference reviewer for NeurIPS, ICLR, ICML

Journal reviewer for IEEE TMM, Neurocomputing

<!-- Latest Gallery
======

<img src="images/gallery1.png" alt="Gallery" width="800">

<img src="images/gallery2.png" alt="Gallery" width="800"> -->

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=g9SZs5u3hwA_RMZqdhNPB716AxHRJQYXSMQDCKNg77g&cl=ffffff&w=a"></script>

<!-- Getting started

Site-wide configuration
------
The main configuration file for the site is in the base directory in [_config.yml](https://github.com/academicpages/academicpages.github.io/blob/master/_config.yml), which defines the content in the sidebars and other site-wide features. You will need to replace the default variables with ones about yourself and your site's github repository. The configuration file for the top menu is in [_data/navigation.yml](https://github.com/academicpages/academicpages.github.io/blob/master/_data/navigation.yml). For example, if you don't have a portfolio or blog posts, you can remove those items from that navigation.yml file to remove them from the header. 

Create content & metadata
------
For site content, there is one markdown file for each type of content, which are stored in directories like _publications, _talks, _posts, _teaching, or _pages. For example, each talk is a markdown file in the [_talks directory](https://github.com/academicpages/academicpages.github.io/tree/master/_talks). At the top of each markdown file is structured data in YAML about the talk, which the theme will parse to do lots of cool stuff. The same structured data about a talk is used to generate the list of talks on the [Talks page](https://academicpages.github.io/talks), each [individual page](https://academicpages.github.io/talks/2012-03-01-talk-1) for specific talks, the talks section for the [CV page](https://academicpages.github.io/cv), and the [map of places you've given a talk](https://academicpages.github.io/talkmap.html) (if you run this [python file](https://github.com/academicpages/academicpages.github.io/blob/master/talkmap.py) or [Jupyter notebook](https://github.com/academicpages/academicpages.github.io/blob/master/talkmap.ipynb), which creates the HTML for the map based on the contents of the _talks directory).

**Markdown generator**

I have also created [a set of Jupyter notebooks](https://github.com/academicpages/academicpages.github.io/tree/master/markdown_generator
) that converts a CSV containing structured data about talks or presentations into individual markdown files that will be properly formatted for the academicpages template. The sample CSVs in that directory are the ones I used to create my own personal website at stuartgeiger.com. My usual workflow is that I keep a spreadsheet of my publications and talks, then run the code in these notebooks to generate the markdown files, then commit and push them to the GitHub repository.

How to edit your site's GitHub repository
------
Many people use a git client to create files on their local computer and then push them to GitHub's servers. If you are not familiar with git, you can directly edit these configuration and markdown files directly in the github.com interface. Navigate to a file (like [this one](https://github.com/academicpages/academicpages.github.io/blob/master/_talks/2012-03-01-talk-1.md) and click the pencil icon in the top right of the content preview (to the right of the "Raw | Blame | History" buttons). You can delete a file by clicking the trashcan icon to the right of the pencil icon. You can also create new files or upload files by navigating to a directory and clicking the "Create new file" or "Upload files" buttons. 

Example: editing a markdown file for a talk
![Editing a markdown file for a talk](/images/editing-talk.png)

For more info
------
More info about configuring academicpages can be found in [the guide](https://academicpages.github.io/markdown/). The [guides for the Minimal Mistakes theme](https://mmistakes.github.io/minimal-mistakes/docs/configuration/) (which this theme was forked from) might also be helpful. -->
